#!/usr/bin/env python3
"""
run_benchmark.py
LLM BDD Benchmark ä¸»å…¥å£è„šæœ¬

è¿è¡Œå®Œæ•´çš„LLMå¯¹æ¯”å®éªŒ
"""

import sys
from pathlib import Path

# ============================================================================
# ä»£ç†è®¾ç½® - è‡ªåŠ¨ä»é…ç½®æ–‡ä»¶è¯»å–
# ============================================================================
def setup_proxy():
    """ä»é…ç½®æ–‡ä»¶è¯»å–å¹¶è®¾ç½®ä»£ç†ç¯å¢ƒå˜é‡"""
    import os
    import json
    from pathlib import Path

    # æŸ¥æ‰¾é…ç½®æ–‡ä»¶
    config_paths = [
        Path('config/llm_config.json'),
        Path('llm_config.json'),
        Path('../config/llm_config.json'),
    ]

    config_file = None
    for path in config_paths:
        if path.exists():
            config_file = path
            break

    if not config_file:
        print("âš ï¸  æœªæ‰¾åˆ° llm_config.jsonï¼Œä»£ç†æœªè®¾ç½®")
        return

    try:
        with open(config_file, 'r', encoding='utf-8') as f:
            config = json.load(f)

        proxy_config = config.get('proxy', {})

        if proxy_config.get('enabled'):
            os.environ['HTTP_PROXY'] = proxy_config.get('http_proxy', '')
            os.environ['HTTPS_PROXY'] = proxy_config.get('https_proxy', '')
            print(f"ğŸŒ ä»£ç†å·²å¯ç”¨: {os.environ['HTTPS_PROXY']}")
        else:
            print("â„¹ï¸  ä»£ç†æœªå¯ç”¨ï¼ˆé…ç½®ä¸­ enabled=falseï¼‰")

    except Exception as e:
        print(f"âš ï¸  è¯»å–ä»£ç†é…ç½®å¤±è´¥: {e}")

# è°ƒç”¨ä»£ç†è®¾ç½®
setup_proxy()
# ============================================================================



# ä¿®æ”¹è¿™ä¸¤è¡Œ
sys.path.insert(0, str(Path(__file__).parent / 'src'))  # æ·»åŠ srcåˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))           # æ·»åŠ æ ¹ç›®å½•

# ç„¶åä¿®æ”¹å¯¼å…¥
from benchmark.standard_dut_generator import StandardDUTGenerator
from benchmark.benchmark_runner import BenchmarkRunner
from benchmark.experiment_config import SPECS


def main():
    """ä¸»å‡½æ•°"""

    print("\n" + "=" * 80)
    print("ğŸ¯ LLM BDD Quality Benchmark Experiment")
    print("=" * 80)
    print("\nThis experiment will:")
    print("  1. Generate standard DUTs (once)")
    print("  2. Test multiple LLMs on BDD generation")
    print("  3. Compare quality metrics")
    print("=" * 80 + "\n")

    # ========== Phase 1: ç”Ÿæˆæ ‡å‡†DUTï¼ˆåªåšä¸€æ¬¡ï¼‰==========
    print("ğŸ“‹ Phase 1: Generating Standard DUTs...")
    print("-" * 80)

    dut_generator = StandardDUTGenerator(output_dir="standard_dut")
    dut_map = dut_generator.generate_all_standard_duts(SPECS)

    if not dut_map:
        print("\nâŒ Error: No DUTs generated. Please check your spec files.")
        return 1

    print(f"\nâœ… Phase 1 Complete: Generated {len(dut_map)} standard DUTs")
    print("=" * 80 + "\n")

    # ========== Phase 2: LLM Benchmark ==========
    print("ğŸ“‹ Phase 2: Running LLM Benchmark...")
    print("-" * 80 + "\n")

    runner = BenchmarkRunner(standard_dut_map=dut_map)
    results = runner.run_benchmark()

    print(f"\nâœ… Phase 2 Complete: {len(results)} experiments finished")
    print("=" * 80)

    # ========== ç»“æœä½ç½® ==========
    print("\nğŸ“ Results saved to:")
    print("  - Raw outputs: benchmark_results/raw_outputs/")
    print("  - Metrics: benchmark_results/metrics/intermediate_results.json")
    print("  - Invalid JSON: benchmark_results/invalid/json_errors/")
    print("  - Standard DUTs: standard_dut/")
    print("=" * 80 + "\n")

    return 0


if __name__ == "__main__":
    try:
        exit_code = main()
        sys.exit(exit_code)
    except KeyboardInterrupt:
        print("\n\nâš ï¸  Experiment interrupted by user")
        sys.exit(1)
    except Exception as e:
        print(f"\n\nâŒ Error: {e}")
        import traceback

        traceback.print_exc()
        sys.exit(1)