"""
Metadata Extractor - Extract metadata from Feature files
========================================================

This module reads Feature files generated by feature_generator_llm.py
and extracts structured metadata (JSON + TXT format).

PURPOSE:
- Feature files are the SOURCE (generated by LLM)
- Metadata files are DERIVED (extracted from Feature)
- Provides backward compatibility and analysis convenience

DIRECTORY STRUCTURE:
    output/
    â”œâ”€â”€ bdd/              â† Feature files (source)
    â”‚   â”œâ”€â”€ groq/
    â”‚   â”‚   â””â”€â”€ alu_16bit_xxx.feature
    â”‚   â””â”€â”€ openai/
    â”‚
    â””â”€â”€ metadata/         â† Extracted metadata (derived)
        â”œâ”€â”€ groq/
        â”‚   â”œâ”€â”€ alu_16bit_xxx.json
        â”‚   â””â”€â”€ alu_16bit_xxx.txt
        â””â”€â”€ openai/

USAGE:
    # Extract from all Feature files
    python metadata_extractor.py

    # Extract from specific directory
    python metadata_extractor.py --bdd-dir output/bdd
"""

import re
import json
import argparse
from pathlib import Path
from typing import Dict, List, Optional, Tuple
from datetime import datetime


class MetadataExtractor:
    """
    Extract metadata from Feature files

    Parses .feature files and creates corresponding .json and .txt files
    in the metadata/ directory.
    """

    def __init__(
            self,
            bdd_dir: Optional[str] = None,
            metadata_dir: Optional[str] = None,
            project_root: Optional[str] = None,
            debug: bool = True
    ):
        """
        Initialize Metadata Extractor

        Args:
            bdd_dir: Directory containing Feature files
            metadata_dir: Output directory for metadata
            project_root: Project root directory
            debug: Enable debug output
        """
        self.debug = debug

        # Setup paths
        self.bdd_base_dir = self._find_bdd_dir(bdd_dir, project_root)
        self.metadata_base_dir = self._setup_metadata_dir(metadata_dir, project_root)

        print(f"ğŸ“ Feature files: {self.bdd_base_dir}")
        print(f"ğŸ“ Metadata output: {self.metadata_base_dir}")

    def _find_bdd_dir(self, bdd_dir: Optional[str], project_root: Optional[str]) -> Path:
        """Find BDD directory containing Feature files"""
        if bdd_dir:
            return Path(bdd_dir)

        if project_root:
            path = Path(project_root) / "output" / "bdd"
            if path.exists():
                return path

        # Search common locations
        current = Path.cwd()
        possible_paths = [
            current / "output" / "bdd",
            current / "bdd",
            current.parent / "output" / "bdd",
        ]

        for path in possible_paths:
            if path.exists():
                return path

        # Default
        default = current / "output" / "bdd"
        default.mkdir(parents=True, exist_ok=True)
        return default

    def _setup_metadata_dir(self, metadata_dir: Optional[str], project_root: Optional[str]) -> Path:
        """Setup metadata output directory"""
        if metadata_dir:
            path = Path(metadata_dir)
        elif project_root:
            path = Path(project_root) / "output" / "metadata"
        else:
            # Same level as bdd
            path = self.bdd_base_dir.parent / "metadata"

        path.mkdir(parents=True, exist_ok=True)
        return path

    def extract_all(self) -> Dict[str, int]:
        """
        Extract metadata from all Feature files

        Returns:
            Statistics dict
        """
        print(f"\n{'=' * 70}")
        print(f"ğŸ” Metadata Extraction")
        print(f"{'=' * 70}\n")

        stats = {
            'total': 0,
            'success': 0,
            'failed': 0,
            'llm_providers': []
        }

        # Scan for LLM subdirectories
        llm_dirs = [d for d in self.bdd_base_dir.iterdir() if d.is_dir()]

        if not llm_dirs:
            print("âš ï¸  No LLM subdirectories found")
            return stats

        print(f"ğŸ“‚ Found {len(llm_dirs)} LLM provider(s):\n")

        for llm_dir in llm_dirs:
            llm_name = llm_dir.name
            print(f"   ğŸ“ {llm_name}/")
            stats['llm_providers'].append(llm_name)

            # Find Feature files
            feature_files = list(llm_dir.glob("*.feature"))

            if not feature_files:
                print(f"      âš ï¸  No .feature files found")
                continue

            print(f"      Found {len(feature_files)} feature file(s)")

            # Create metadata subdirectory
            metadata_llm_dir = self.metadata_base_dir / llm_name
            metadata_llm_dir.mkdir(parents=True, exist_ok=True)

            # Extract from each feature
            for feature_path in feature_files:
                stats['total'] += 1
                try:
                    self._extract_from_feature(feature_path, metadata_llm_dir)
                    stats['success'] += 1
                    print(f"      âœ… {feature_path.name}")
                except Exception as e:
                    stats['failed'] += 1
                    print(f"      âŒ {feature_path.name}: {e}")
                    if self.debug:
                        import traceback
                        traceback.print_exc()

            print()

        # Summary
        print(f"{'=' * 70}")
        print(f"âœ¨ Extraction Complete")
        print(f"{'=' * 70}\n")
        print(f"ğŸ“Š Statistics:")
        print(f"   Total files: {stats['total']}")
        print(f"   Success: {stats['success']}")
        print(f"   Failed: {stats['failed']}")
        print(f"   LLM providers: {len(stats['llm_providers'])}")
        print()

        return stats

    def _extract_from_feature(self, feature_path: Path, output_dir: Path):
        """
        Extract metadata from a single Feature file

        Args:
            feature_path: Path to .feature file
            output_dir: Output directory for metadata
        """
        # Read feature content
        with open(feature_path, 'r', encoding='utf-8') as f:
            content = f.read()

        # Extract metadata
        metadata = self._parse_feature(content, feature_path)

        # Generate filenames (same stem as feature file)
        base_name = feature_path.stem
        json_path = output_dir / f"{base_name}.json"
        txt_path = output_dir / f"{base_name}.txt"

        # Save JSON
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, indent=2, ensure_ascii=False)

        # Save TXT
        txt_content = self._format_metadata_txt(metadata)
        with open(txt_path, 'w', encoding='utf-8') as f:
            f.write(txt_content)

    def _parse_feature(self, content: str, feature_path: Path) -> Dict:
        """
        Parse Feature file and extract metadata

        Args:
            content: Feature file content
            feature_path: Path to feature file (for metadata)

        Returns:
            Metadata dictionary
        """
        # Extract bitwidth
        bitwidth = 16  # default
        bitwidth_match = re.search(r'(\d+)[-_]?bit', content, re.IGNORECASE)
        if bitwidth_match:
            bitwidth = int(bitwidth_match.group(1))

        # Extract LLM provider from comments
        llm_provider = 'unknown'
        source_match = re.search(r'#\s*Source:\s*(\w+)', content)
        if source_match:
            llm_provider = source_match.group(1)
        else:
            # Try from directory name
            llm_provider = feature_path.parent.name

        # Extract timestamp from filename or comments
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        timestamp_match = re.search(r'(\d{8}_\d{6})', feature_path.name)
        if timestamp_match:
            timestamp = timestamp_match.group(1)
        else:
            generated_match = re.search(r'#\s*Generated:\s*(\d{8}_\d{6})', content)
            if generated_match:
                timestamp = generated_match.group(1)

        # Extract operations
        operations = {}

        # Find all Scenario Outline sections
        scenario_pattern = r'Scenario Outline:\s*Verify\s+(\w+)\s+operation.*?opcode\s+(\d{4})'
        for match in re.finditer(scenario_pattern, content, re.IGNORECASE):
            op_name = match.group(1).upper()
            opcode = match.group(2)

            # Try to extract description
            desc_pattern = rf'#\s*(.*?{op_name}.*?)$'
            desc_match = re.search(desc_pattern, content, re.MULTILINE | re.IGNORECASE)
            description = desc_match.group(1).strip() if desc_match else f"{op_name} operation"

            operations[op_name] = {
                'opcode': opcode,
                'description': description
            }

        # Extract test cases
        test_cases = self._extract_test_cases(content)

        # Count tests per operation
        tests_per_operation = {}
        for test in test_cases:
            op = test.get('operation', 'UNKNOWN')
            tests_per_operation[op] = tests_per_operation.get(op, 0) + 1

        # Build metadata
        metadata = {
            'bitwidth': bitwidth,
            'operations': operations,
            'total_tests': len(test_cases),
            'tests_per_operation': tests_per_operation,
            'test_cases': test_cases,
            'metadata': {
                'llm_provider': llm_provider,
                'timestamp': timestamp,
                'generated_by': 'feature_generator_llm',
                'extracted_by': 'metadata_extractor',
                'extraction_date': datetime.now().strftime("%Y%m%d_%H%M%S"),
                'source_file': feature_path.name
            }
        }

        return metadata

    def _extract_test_cases(self, content: str) -> List[Dict]:
        """
        Extract test cases from Examples tables

        Args:
            content: Feature file content

        Returns:
            List of test case dictionaries
        """
        test_cases = []

        # Find all Examples tables
        examples_pattern = r'Examples:\s*\n((?:\s*\|.*\n)+)'

        for match in re.finditer(examples_pattern, content, re.MULTILINE):
            table_text = match.group(1)
            rows = [row.strip() for row in table_text.strip().split('\n')]

            if len(rows) < 2:
                continue

            # Parse header
            header = [col.strip() for col in rows[0].split('|') if col.strip()]

            # Parse data rows
            for row in rows[1:]:
                cols = [col.strip() for col in row.split('|') if col.strip()]

                if len(cols) != len(header):
                    continue

                test_case = {}
                for col_name, col_value in zip(header, cols):
                    # Try to parse as integer
                    try:
                        test_case[col_name.lower()] = int(col_value)
                    except ValueError:
                        # Keep as string (e.g., "True", "False")
                        test_case[col_name.lower()] = col_value

                # Try to infer operation from context
                # Look backward for Scenario Outline
                scenarios = re.finditer(r'Scenario Outline:\s*Verify\s+(\w+)', content)
                current_op = None
                for scenario_match in scenarios:
                    if scenario_match.start() < match.start():
                        current_op = scenario_match.group(1).upper()

                if current_op:
                    test_case['operation'] = current_op

                test_cases.append(test_case)

        return test_cases

    def _format_metadata_txt(self, metadata: Dict) -> str:
        """
        Format metadata as human-readable text

        Args:
            metadata: Metadata dictionary

        Returns:
            Formatted text content
        """
        lines = []
        lines.append("=" * 80)
        lines.append("ALU FEATURE METADATA")
        lines.append("=" * 80)
        lines.append("")

        meta = metadata['metadata']
        lines.append(f"Source Feature: {meta['source_file']}")
        lines.append(f"LLM Provider: {meta['llm_provider']}")
        lines.append(f"Generated: {meta['timestamp']}")
        lines.append(f"Extracted: {meta['extraction_date']}")
        lines.append("")

        lines.append(f"BITWIDTH: {metadata['bitwidth']}")
        lines.append("")

        lines.append("OPERATIONS:")
        lines.append("-" * 80)
        for op_name, op_info in metadata['operations'].items():
            lines.append(f"  {op_name:<6} (opcode: {op_info['opcode']})")
            lines.append(f"         {op_info['description']}")
            lines.append("")

        lines.append("TEST STATISTICS:")
        lines.append("-" * 80)
        lines.append(f"  Total tests: {metadata['total_tests']}")
        lines.append("")
        lines.append("  Tests per operation:")
        for op, count in sorted(metadata['tests_per_operation'].items()):
            lines.append(f"    â€¢ {op}: {count} tests")
        lines.append("")

        lines.append("=" * 80)
        lines.append(f"Note: This metadata is extracted from the Feature file.")
        lines.append(f"      The Feature file is the authoritative source.")
        lines.append("=" * 80)

        return '\n'.join(lines)


def main():
    """Main entry point"""
    parser = argparse.ArgumentParser(
        description='Extract metadata from Feature files',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog='''
Examples:
  # Extract from all Feature files
  python metadata_extractor.py

  # Specify directories
  python metadata_extractor.py --bdd-dir output/bdd --metadata-dir output/metadata

  # With project root
  python metadata_extractor.py --project-root D:/MyProject
        '''
    )

    parser.add_argument('--bdd-dir', help='Directory containing Feature files')
    parser.add_argument('--metadata-dir', help='Output directory for metadata')
    parser.add_argument('--project-root', help='Project root directory')
    parser.add_argument('--debug', action='store_true', default=True,
                        help='Enable debug output')

    args = parser.parse_args()

    # Create extractor
    extractor = MetadataExtractor(
        bdd_dir=args.bdd_dir,
        metadata_dir=args.metadata_dir,
        project_root=args.project_root,
        debug=args.debug
    )

    # Extract metadata
    stats = extractor.extract_all()

    if stats['success'] > 0:
        print(f"ğŸ‰ Successfully extracted metadata from {stats['success']} file(s)")
        print(f"\nğŸ“‹ Next steps:")
        print(f"   1. Review metadata in: {extractor.metadata_base_dir}")
        print(f"   2. Generate testbenches: python testbench_generator.py")
        return 0
    else:
        print(f"âŒ No metadata extracted")
        return 1


if __name__ == "__main__":
    import sys

    sys.exit(main())