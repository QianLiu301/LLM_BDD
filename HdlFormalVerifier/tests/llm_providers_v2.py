"""
LLM Provider Module
Supports multiple LLM APIs including FREE options for generating BDD scenario descriptions
"""

import os
import json
import time
from abc import ABC, abstractmethod
from typing import Dict, List, Optional
import requests

# å°è¯•å¯¼å…¥ google-genaiï¼Œå¦‚æœå¤±è´¥åˆ™æ ‡è®°
try:
    from google import genai
    from google.genai import errors
    GENAI_AVAILABLE = True
except ImportError:
    GENAI_AVAILABLE = False
    print("âš ï¸  google-genai not installed. Install with: pip install -U google-genai")

# å°è¯•å¯¼å…¥ openai SDKï¼Œå¦‚æœå¤±è´¥åˆ™æ ‡è®°
try:
    from openai import OpenAI
    OPENAI_SDK_AVAILABLE = True
except ImportError:
    OPENAI_SDK_AVAILABLE = False
    print("âš ï¸  openai SDK not installed. Install with: pip install openai")


class LLMProvider(ABC):
    """Abstract base class for LLM providers"""

    @abstractmethod
    def generate_scenario_description(
        self,
        operation_name: str,
        operation_code: str,
        operation_description: str,
        bitwidth: int
    ) -> str:
        """Generate a BDD scenario description"""
        pass

    @abstractmethod
    def generate_feature_description(
        self,
        bitwidth: int,
        operations_count: int,
        operations_list: List[str]
    ) -> str:
        """Generate a Feature-level description"""
        pass


# ========== FREE PROVIDERS ==========


class GeminiProvider(LLMProvider):
    """
    Google Gemini API Provider - FREE!

    Free tier: 60 requests per minute
    How to get API key:
    1. Visit: https://makersuite.google.com/app/apikey
    2. Click "Create API Key"
    3. Copy the key

    Note: Requires Google account
    Now uses google-genai SDK with retry and model fallback
    """

    def __init__(self, api_key: Optional[str] = None, model: str = None):
        self.api_key = api_key or os.getenv("GEMINI_API_KEY")

        if not self.api_key:
            raise ValueError("Gemini API key not provided. Get free key at: https://makersuite.google.com/app/apikey")

        # å¦‚æœ google-genai å¯ç”¨ï¼Œä½¿ç”¨æ–°çš„ SDK
        if GENAI_AVAILABLE:
            # ä½¿ç”¨æ–°çš„æ¨¡å‹åç§°å’Œ SDK
            self.models_to_try = ["gemini-2.5-flash", "gemini-1.5-flash"]
            self.client = genai.Client(api_key=self.api_key)
            self.use_sdk = True
            self.max_retries = 3
            self.sleep_seconds = 2.0
        else:
            # é™çº§åˆ°æ—§çš„ REST API æ–¹å¼
            self.model = model or "gemini-1.5-flash"
            self.use_sdk = False
            print("âš ï¸  Using REST API fallback. For better reliability, install: pip install -U google-genai")

    def _call_api_sdk(self, prompt: str) -> str:
        """ä½¿ç”¨æ–°çš„ google-genai SDK è°ƒç”¨ APIï¼ŒåŒ…å«é‡è¯•å’Œæ¨¡å‹é™çº§"""
        last_error = None

        for model_name in self.models_to_try:
            for attempt in range(1, self.max_retries + 1):
                try:
                    response = self.client.models.generate_content(
                        model=model_name,
                        contents=prompt,
                    )
                    return response.text

                except errors.ServerError as e:
                    # å¤„ç† 5xx é”™è¯¯ï¼ˆåŒ…æ‹¬ 503 overloadedï¼‰
                    last_error = e
                    if attempt < self.max_retries:
                        time.sleep(self.sleep_seconds)
                    else:
                        break  # è¿™ä¸ªæ¨¡å‹çš„é‡è¯•æ¬¡æ•°ç”¨å®Œäº†

                except Exception as e:
                    # å…¶ä»–é”™è¯¯ï¼Œä¸éœ€è¦é‡è¯•
                    last_error = e
                    break

            # å¦‚æœä¸æ˜¯æœ€åä¸€ä¸ªæ¨¡å‹ï¼Œç»§ç»­å°è¯•ä¸‹ä¸€ä¸ª
            if model_name != self.models_to_try[-1]:
                continue

        # æ‰€æœ‰å°è¯•å¤±è´¥ï¼Œè¿”å› fallback
        print(f"âš ï¸  Gemini API failed after all attempts: {last_error}")
        return self._fallback_description(prompt)

    def _call_api_rest(self, prompt: str, max_tokens: int = 200) -> str:
        """æ—§çš„ REST API è°ƒç”¨æ–¹å¼ï¼ˆä½œä¸ºå¤‡ç”¨ï¼‰"""
        url = f"https://generativelanguage.googleapis.com/v1beta/models/{self.model}:generateContent?key={self.api_key}"

        payload = {
            "contents": [{
                "parts": [{
                    "text": prompt
                }]
            }],
            "generationConfig": {
                "maxOutputTokens": max_tokens,
                "temperature": 0.7,
            }
        }

        try:
            response = requests.post(url, json=payload, timeout=30)
            response.raise_for_status()
            result = response.json()
            return result['candidates'][0]['content']['parts'][0]['text'].strip()
        except Exception as e:
            print(f"âš ï¸  Gemini REST API request failed: {e}")
            return self._fallback_description(prompt)

    def _call_api(self, prompt: str, max_tokens: int = 200) -> str:
        """ç»Ÿä¸€çš„ API è°ƒç”¨æ¥å£"""
        if self.use_sdk:
            return self._call_api_sdk(prompt)
        else:
            return self._call_api_rest(prompt, max_tokens)

    def generate_scenario_description(
        self,
        operation_name: str,
        operation_code: str,
        operation_description: str,
        bitwidth: int
    ) -> str:
        """Generate a BDD scenario description"""
        prompt = f"""Generate a clear BDD scenario description for testing a {bitwidth}-bit ALU operation.

Operation Details:
- Name: {operation_name}
- Opcode: {operation_code}
- Description: {operation_description}
- Bitwidth: {bitwidth} bits

Requirements:
1. Write in Gherkin/BDD style
2. Be specific about the operation being tested
3. Keep it concise (1-2 sentences)
4. Focus on functional behavior

Generate the scenario description:
"""
        return self._call_api(prompt, max_tokens=150)

    def generate_feature_description(
        self,
        bitwidth: int,
        operations_count: int,
        operations_list: List[str]
    ) -> str:
        """Generate a Feature-level description"""
        prompt = f"""Generate a BDD Feature description for a {bitwidth}-bit ALU verification suite.

ALU Details:
- Bitwidth: {bitwidth} bits
- Total Operations: {operations_count}
- Operations: {', '.join(operations_list[:10])}

Generate a concise Feature description (2-4 sentences):
"""
        return self._call_api(prompt, max_tokens=200)

    def _fallback_description(self, prompt: str) -> str:
        """Fallback description when API fails"""
        return "Test ALU operation with various input values and verify correct output"


class GroqProvider(LLMProvider):
    """
    Groq API Provider - FREE and FAST!

    Free tier: 30 requests per minute, 14,400 per day
    How to get API key:
    1. Visit: https://console.groq.com/keys
    2. Sign up (free, no credit card needed)
    3. Create API key

    Models: llama3-70b, mixtral-8x7b, gemma-7b
    """

    def __init__(self, api_key: Optional[str] = None, model: str = "llama-3.3-70b-versatile"):
        self.api_key = api_key or os.getenv("GROQ_API_KEY")
        self.model = model
        self.api_url = "https://api.groq.com/openai/v1/chat/completions"

        if not self.api_key:
            raise ValueError("Groq API key not provided. Get free key at: https://console.groq.com/keys")

    def _call_api(self, prompt: str, max_tokens: int = 200) -> str:
        """Call Groq API"""
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }

        payload = {
            "model": self.model,
            "messages": [
                {
                    "role": "system",
                    "content": "You are a helpful assistant that generates clear, concise BDD scenario descriptions for hardware verification."
                },
                {
                    "role": "user",
                    "content": prompt
                }
            ],
            "max_tokens": max_tokens,
            "temperature": 0.7
        }

        try:
            response = requests.post(self.api_url, headers=headers, json=payload, timeout=30)
            response.raise_for_status()
            result = response.json()
            return result['choices'][0]['message']['content'].strip()
        except Exception as e:
            print(f"âš ï¸  Groq API request failed: {e}")
            return self._fallback_description(prompt)

    def generate_scenario_description(
        self,
        operation_name: str,
        operation_code: str,
        operation_description: str,
        bitwidth: int
    ) -> str:
        """Generate a BDD scenario description"""
        prompt = f"""Generate a clear BDD scenario description for testing a {bitwidth}-bit ALU operation.

Operation: {operation_name} (Opcode: {operation_code})
Description: {operation_description}
Bitwidth: {bitwidth} bits

Generate a concise BDD scenario description (1-2 sentences):
"""
        return self._call_api(prompt, max_tokens=150)

    def generate_feature_description(
        self,
        bitwidth: int,
        operations_count: int,
        operations_list: List[str]
    ) -> str:
        """Generate a Feature-level description"""
        prompt = f"""Generate a BDD Feature description for a {bitwidth}-bit ALU with {operations_count} operations including {', '.join(operations_list[:5])}.

Generate a concise Feature description (2-3 sentences):
"""
        return self._call_api(prompt, max_tokens=200)

    def _fallback_description(self, prompt: str) -> str:
        """Fallback description when API fails"""
        return "Verify ALU operation performs correctly with expected results"


class DeepSeekProvider(LLMProvider):
    """
    DeepSeek API Provider - Chinese LLM with free tier

    Free tier: Available with registration
    How to get API key:
    1. Visit: https://platform.deepseek.com/
    2. Sign up (requires Chinese phone number)
    3. Get API key
    """

    def __init__(self, api_key: Optional[str] = None, model: str = "deepseek-chat"):
        self.api_key = api_key or os.getenv("DEEPSEEK_API_KEY")
        self.model = model
        self.api_url = "https://api.deepseek.com/v1/chat/completions"

        if not self.api_key:
            raise ValueError("DeepSeek API key not provided. Get key at: https://platform.deepseek.com/")

    def _call_api(self, prompt: str, max_tokens: int = 200) -> str:
        """Call DeepSeek API"""
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }

        payload = {
            "model": self.model,
            "messages": [
                {"role": "system", "content": "You are a helpful assistant for hardware verification."},
                {"role": "user", "content": prompt}
            ],
            "max_tokens": max_tokens,
            "temperature": 0.7
        }

        try:
            response = requests.post(self.api_url, headers=headers, json=payload, timeout=30)
            response.raise_for_status()
            result = response.json()
            return result['choices'][0]['message']['content'].strip()
        except Exception as e:
            print(f"âš ï¸  DeepSeek API request failed: {e}")
            return self._fallback_description(prompt)

    def generate_scenario_description(self, operation_name: str, operation_code: str,
                                     operation_description: str, bitwidth: int) -> str:
        prompt = f"Generate a BDD scenario description for {bitwidth}-bit ALU {operation_name} operation (opcode: {operation_code}). Keep it concise."
        return self._call_api(prompt, max_tokens=150)

    def generate_feature_description(self, bitwidth: int, operations_count: int,
                                    operations_list: List[str]) -> str:
        prompt = f"Generate a Feature description for {bitwidth}-bit ALU with {operations_count} operations. Keep it concise."
        return self._call_api(prompt, max_tokens=200)

    def _fallback_description(self, prompt: str) -> str:
        return "Test ALU operation with various inputs"


# ========== PAID PROVIDERS (Original) ==========
class OpenAIProvider(LLMProvider):
    """
    OpenAI GPT provider (PAID - requires credits)

    Now supports two modes:
    1. Official SDK (recommended) - more stable, better error handling
    2. REST API (fallback) - when SDK is unavailable
    """

    def __init__(self, api_key: Optional[str] = None, model: str = "gpt-4o-mini"):
        self.api_key = api_key or os.getenv("OPENAI_API_KEY")
        self.model = model

        if not self.api_key:
            raise ValueError("OpenAI API key not provided")

        # å¦‚æœ SDK å¯ç”¨ï¼Œä½¿ç”¨ SDKï¼›å¦åˆ™ä½¿ç”¨ REST API
        if OPENAI_SDK_AVAILABLE:
            self.client = OpenAI(api_key=self.api_key)
            self.use_sdk = True
            print("ğŸ¯ Using OpenAI SDK (recommended)")
            # ğŸ” è°ƒè¯•ï¼šæ‰“å° base_url å’Œé»˜è®¤æ¨¡å‹
            try:
                print(f"ğŸ” [DEBUG] OpenAI base_url: {self.client.base_url}")
            except Exception:
                print("âš ï¸ [DEBUG] Cannot read client.base_url (SDK version difference)")
        else:
            self.api_url = "https://api.openai.com/v1/chat/completions"
            self.use_sdk = False
            print("âš ï¸  Using REST API fallback. For better reliability, install: pip install openai")

    def _call_api_sdk(self, prompt: str, max_tokens: int = 200) -> str:
        """ä½¿ç”¨å®˜æ–¹ OpenAI SDK è°ƒç”¨ API"""
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {
                        "role": "system",
                        "content": "You are a helpful assistant that generates clear, concise BDD scenario descriptions for hardware verification."
                    },
                    {
                        "role": "user",
                        "content": prompt
                    }
                ],
                max_tokens=max_tokens,
                temperature=0.7
            )

            # === ğŸ” DEBUG: ç¡®è®¤çœŸçš„åœ¨ç”¨ OpenAIï¼Œå¹¶æŸ¥çœ‹è´¹ç”¨æƒ…å†µ ===
            try:
                print("ğŸ”— [DEBUG][OpenAI SDK] Response ID:", response.id)
                print("ğŸ¤– [DEBUG][OpenAI SDK] Model:", response.model)

                if hasattr(response, "usage") and response.usage is not None:
                    print(
                        "ğŸ’° [DEBUG][OpenAI SDK] Tokens - "
                        f"Prompt: {response.usage.prompt_tokens}, "
                        f"Completion: {response.usage.completion_tokens}, "
                        f"Total: {response.usage.total_tokens}"
                    )
                else:
                    print("âš ï¸ [DEBUG][OpenAI SDK] No usage info in response (maybe older API / special model)")
            except Exception as dbg_e:
                print(f"âš ï¸ [DEBUG][OpenAI SDK] Failed to read debug info: {dbg_e}")
            # ========================================================

            return response.choices[0].message.content.strip()

        except Exception as e:
            print(f"âš ï¸  OpenAI SDK request failed: {e}")
            return self._fallback_description(prompt)

    def _call_api_rest(self, prompt: str, max_tokens: int = 200) -> str:
        """ä½¿ç”¨ REST API è°ƒç”¨ï¼ˆå¤‡ç”¨æ–¹æ¡ˆï¼‰"""
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }

        payload = {
            "model": self.model,
            "messages": [
                {
                    "role": "system",
                    "content": "You are a helpful assistant that generates clear, concise BDD scenario descriptions for hardware verification."
                },
                {
                    "role": "user",
                    "content": prompt
                }
            ],
            "max_tokens": max_tokens,
            "temperature": 0.7
        }

        try:
            response = requests.post(
                self.api_url,
                headers=headers,
                json=payload,
                timeout=30
            )
            response.raise_for_status()
            result = response.json()

            # === ğŸ” DEBUG: REST æ¨¡å¼ä¸‹çš„è°ƒè¯•è¾“å‡º ===
            try:
                print("ğŸ”— [DEBUG][OpenAI REST] HTTP Status:", response.status_code)
                print("ğŸ¤– [DEBUG][OpenAI REST] Model:", result.get("model"))
                usage = result.get("usage", {})
                if usage:
                    print(
                        "ğŸ’° [DEBUG][OpenAI REST] Tokens - "
                        f"Prompt: {usage.get('prompt_tokens')}, "
                        f"Completion: {usage.get('completion_tokens')}, "
                        f"Total: {usage.get('total_tokens')}"
                    )
                else:
                    print("âš ï¸ [DEBUG][OpenAI REST] No usage info in JSON response")
            except Exception as dbg_e:
                print(f"âš ï¸ [DEBUG][OpenAI REST] Failed to read debug info: {dbg_e}")
            # =====================================================

            return result['choices'][0]['message']['content'].strip()
        except Exception as e:
            print(f"âš ï¸  OpenAI REST API request failed: {e}")
            return self._fallback_description(prompt)

    def _call_api(self, prompt: str, max_tokens: int = 200) -> str:
        """ç»Ÿä¸€çš„ API è°ƒç”¨æ¥å£"""
        if self.use_sdk:
            return self._call_api_sdk(prompt, max_tokens)
        else:
            return self._call_api_rest(prompt, max_tokens)

    def generate_scenario_description(
        self,
        operation_name: str,
        operation_code: str,
        operation_description: str,
        bitwidth: int
    ) -> str:
        """Generate a BDD scenario description"""
        prompt = f"""Generate a clear BDD scenario description for testing a {bitwidth}-bit ALU operation.

Operation Details:
- Name: {operation_name}
- Opcode: {operation_code}
- Description: {operation_description}
- Bitwidth: {bitwidth} bits

Requirements:
1. Write in Gherkin/BDD style
2. Be specific about the operation being tested
3. Keep it concise (1-2 sentences)
4. Focus on functional behavior

Generate the scenario description:
"""
        return self._call_api(prompt, max_tokens=150)

    def generate_feature_description(
        self,
        bitwidth: int,
        operations_count: int,
        operations_list: List[str]
    ) -> str:
        """Generate a Feature-level description"""
        prompt = f"""Generate a BDD Feature description for a {bitwidth}-bit ALU verification suite.

ALU Details:
- Bitwidth: {bitwidth} bits
- Total Operations: {operations_count}
- Operations: {', '.join(operations_list[:10])}

Generate the Feature description:
"""
        return self._call_api(prompt, max_tokens=200)

    def _fallback_description(self, prompt: str) -> str:
        """Fallback description when API fails"""
        return "Test ALU operation with various input values and verify correct output"



# class OpenAIProvider(LLMProvider):
#     """
#     OpenAI GPT provider (PAID - requires credits)
#
#     Now supports two modes:
#     1. Official SDK (recommended) - more stable, better error handling
#     2. REST API (fallback) - when SDK is unavailable
#     """
#
#     def __init__(self, api_key: Optional[str] = None, model: str = "gpt-4o-mini"):
#         self.api_key = api_key or os.getenv("OPENAI_API_KEY")
#         self.model = model
#
#         if not self.api_key:
#             raise ValueError("OpenAI API key not provided")
#
#         # å¦‚æœ SDK å¯ç”¨ï¼Œä½¿ç”¨ SDKï¼›å¦åˆ™ä½¿ç”¨ REST API
#         if OPENAI_SDK_AVAILABLE:
#             self.client = OpenAI(api_key=self.api_key)
#             self.use_sdk = True
#             print("ğŸ¯ Using OpenAI SDK (recommended)")
#         else:
#             self.api_url = "https://api.openai.com/v1/chat/completions"
#             self.use_sdk = False
#             print("âš ï¸  Using REST API fallback. For better reliability, install: pip install openai")
#
#     def _call_api_sdk(self, prompt: str, max_tokens: int = 200) -> str:
#         """ä½¿ç”¨å®˜æ–¹ OpenAI SDK è°ƒç”¨ API"""
#         try:
#             response = self.client.chat.completions.create(
#                 model=self.model,
#                 messages=[
#                     {
#                         "role": "system",
#                         "content": "You are a helpful assistant that generates clear, concise BDD scenario descriptions for hardware verification."
#                     },
#                     {
#                         "role": "user",
#                         "content": prompt
#                     }
#                 ],
#                 max_tokens=max_tokens,
#                 temperature=0.7
#             )
#             return response.choices[0].message.content.strip()
#
#         except Exception as e:
#             print(f"âš ï¸  OpenAI SDK request failed: {e}")
#             return self._fallback_description(prompt)
#
#     def _call_api_rest(self, prompt: str, max_tokens: int = 200) -> str:
#         """ä½¿ç”¨ REST API è°ƒç”¨ï¼ˆå¤‡ç”¨æ–¹æ¡ˆï¼‰"""
#         headers = {
#             "Authorization": f"Bearer {self.api_key}",
#             "Content-Type": "application/json"
#         }
#
#         payload = {
#             "model": self.model,
#             "messages": [
#                 {
#                     "role": "system",
#                     "content": "You are a helpful assistant that generates clear, concise BDD scenario descriptions for hardware verification."
#                 },
#                 {
#                     "role": "user",
#                     "content": prompt
#                 }
#             ],
#             "max_tokens": max_tokens,
#             "temperature": 0.7
#         }
#
#         try:
#             response = requests.post(
#                 self.api_url,
#                 headers=headers,
#                 json=payload,
#                 timeout=30
#             )
#             response.raise_for_status()
#             result = response.json()
#             return result['choices'][0]['message']['content'].strip()
#         except Exception as e:
#             print(f"âš ï¸  OpenAI REST API request failed: {e}")
#             return self._fallback_description(prompt)
#
#     def _call_api(self, prompt: str, max_tokens: int = 200) -> str:
#         """ç»Ÿä¸€çš„ API è°ƒç”¨æ¥å£"""
#         if self.use_sdk:
#             return self._call_api_sdk(prompt, max_tokens)
#         else:
#             return self._call_api_rest(prompt, max_tokens)
#
#     def generate_scenario_description(
#         self,
#         operation_name: str,
#         operation_code: str,
#         operation_description: str,
#         bitwidth: int
#     ) -> str:
#         """Generate a BDD scenario description"""
#         prompt = f"""Generate a clear BDD scenario description for testing a {bitwidth}-bit ALU operation.
#
# Operation Details:
# - Name: {operation_name}
# - Opcode: {operation_code}
# - Description: {operation_description}
# - Bitwidth: {bitwidth} bits
#
# Requirements:
# 1. Write in Gherkin/BDD style
# 2. Be specific about the operation being tested
# 3. Keep it concise (1-2 sentences)
# 4. Focus on functional behavior
#
# Generate the scenario description:
# """
#         return self._call_api(prompt, max_tokens=150)
#
#     def generate_feature_description(
#         self,
#         bitwidth: int,
#         operations_count: int,
#         operations_list: List[str]
#     ) -> str:
#         """Generate a Feature-level description"""
#         prompt = f"""Generate a BDD Feature description for a {bitwidth}-bit ALU verification suite.
#
# ALU Details:
# - Bitwidth: {bitwidth} bits
# - Total Operations: {operations_count}
# - Operations: {', '.join(operations_list[:10])}
#
# Generate the Feature description:
# """
#         return self._call_api(prompt, max_tokens=200)
#
#     def _fallback_description(self, prompt: str) -> str:
#         """Fallback description when API fails"""
#         return "Test ALU operation with various input values and verify correct output"


class ClaudeProvider(LLMProvider):
    """Anthropic Claude provider (PAID - requires credits)"""

    def __init__(self, api_key: Optional[str] = None, model: str = "claude-sonnet-4-20250514"):
        self.api_key = api_key or os.getenv("ANTHROPIC_API_KEY")
        self.model = model
        self.api_url = "https://api.anthropic.com/v1/messages"

        if not self.api_key:
            raise ValueError("Anthropic API key not provided")

    def _call_api(self, prompt: str, max_tokens: int = 200) -> str:
        """Call Claude API"""
        headers = {
            "x-api-key": self.api_key,
            "Content-Type": "application/json",
            "anthropic-version": "2023-06-01"
        }

        payload = {
            "model": self.model,
            "max_tokens": max_tokens,
            "messages": [
                {
                    "role": "user",
                    "content": prompt
                }
            ]
        }

        try:
            response = requests.post(
                self.api_url,
                headers=headers,
                json=payload,
                timeout=30
            )
            response.raise_for_status()
            result = response.json()
            return result['content'][0]['text'].strip()
        except Exception as e:
            print(f"âš ï¸  Claude API request failed: {e}")
            return self._fallback_description(prompt)

    def generate_scenario_description(
        self,
        operation_name: str,
        operation_code: str,
        operation_description: str,
        bitwidth: int
    ) -> str:
        """Generate a BDD scenario description"""
        prompt = f"""Generate a clear BDD scenario description for testing a {bitwidth}-bit ALU operation.

Operation: {operation_name} (Opcode: {operation_code})
Description: {operation_description}

Generate a concise BDD scenario description (1-2 sentences):
"""
        return self._call_api(prompt, max_tokens=150)

    def generate_feature_description(
        self,
        bitwidth: int,
        operations_count: int,
        operations_list: List[str]
    ) -> str:
        """Generate a Feature-level description"""
        prompt = f"""Generate a professional BDD Feature description for a {bitwidth}-bit ALU with {operations_count} operations.

Main operations: {', '.join(operations_list[:10])}

Generate a concise Feature description (2-3 sentences):
"""
        return self._call_api(prompt, max_tokens=200)

    def _fallback_description(self, prompt: str) -> str:
        """Fallback description when API fails"""
        return "Verify ALU operation performs correctly with expected results"


# ========== LOCAL PROVIDER ==========


class LocalLLMProvider(LLMProvider):
    """Local LLM provider using templates (FREE - no API required)"""

    def generate_scenario_description(
        self,
        operation_name: str,
        operation_code: str,
        operation_description: str,
        bitwidth: int
    ) -> str:
        """Generate template-based scenario description"""
        templates = {
            'ADD': f"Verify that the {bitwidth}-bit ALU correctly performs addition of two operands",
            'SUB': f"Verify that the {bitwidth}-bit ALU correctly performs subtraction operation",
            'AND': f"Verify that the {bitwidth}-bit ALU correctly performs bitwise AND operation",
            'OR': f"Verify that the {bitwidth}-bit ALU correctly performs bitwise OR operation",
            'XOR': f"Verify that the {bitwidth}-bit ALU correctly performs bitwise XOR operation",
            'NOT': f"Verify that the {bitwidth}-bit ALU correctly performs bitwise NOT operation",
            'SHL': f"Verify that the {bitwidth}-bit ALU correctly performs left shift operation",
            'SHR': f"Verify that the {bitwidth}-bit ALU correctly performs right shift operation",
            'ROL': f"Verify that the {bitwidth}-bit ALU correctly performs rotate left operation",
            'ROR': f"Verify that the {bitwidth}-bit ALU correctly performs rotate right operation",
            'INC': f"Verify that the {bitwidth}-bit ALU correctly increments the operand by 1",
            'DEC': f"Verify that the {bitwidth}-bit ALU correctly decrements the operand by 1",
            'CMP': f"Verify that the {bitwidth}-bit ALU correctly compares two operands and sets flags",
            'NAND': f"Verify that the {bitwidth}-bit ALU correctly performs bitwise NAND operation",
            'NOR': f"Verify that the {bitwidth}-bit ALU correctly performs bitwise NOR operation",
            'PASS': f"Verify that the {bitwidth}-bit ALU correctly passes through the input operand"
        }

        if operation_name in templates:
            return templates[operation_name]
        else:
            return f"Verify that the {bitwidth}-bit ALU correctly performs {operation_name} operation as specified by {operation_description}"

    def generate_feature_description(
        self,
        bitwidth: int,
        operations_count: int,
        operations_list: List[str]
    ) -> str:
        """Generate Feature-level description"""
        return f"""As a hardware verification engineer
I want to verify the {bitwidth}-bit ALU implementation
So that I can ensure it correctly performs all {operations_count} supported arithmetic and logical operations,
including {', '.join(operations_list[:5])}{' and more' if len(operations_list) > 5 else ''}"""


# ========== FACTORY ==========


class LLMFactory:
    """Factory class for creating LLM providers"""

    @staticmethod
    def create_provider(provider_type: str, **kwargs) -> LLMProvider:
        """Create an LLM provider instance"""
        providers = {
            # FREE providers
            'gemini': GeminiProvider,
            'google': GeminiProvider,
            'groq': GroqProvider,
            'deepseek': DeepSeekProvider,
            'local': LocalLLMProvider,

            # PAID providers
            'openai': OpenAIProvider,
            'gpt': OpenAIProvider,
            'chatgpt': OpenAIProvider,
            'claude': ClaudeProvider,
            'anthropic': ClaudeProvider,
        }

        provider_type = provider_type.lower()
        if provider_type not in providers:
            print(f"âš ï¸  Unknown provider type: {provider_type}, using local template provider")
            provider_type = 'local'

        try:
            provider = providers[provider_type](**kwargs)

            # Print provider info
            if provider_type in ['gemini', 'google']:
                if GENAI_AVAILABLE:
                    print("ğŸ†“ Using Google Gemini (FREE) with SDK - retry & fallback enabled")
                else:
                    print("ğŸ†“ Using Google Gemini (FREE) with REST API")
            elif provider_type == 'groq':
                print("ğŸ†“ Using Groq (FREE and FAST)")
            elif provider_type == 'deepseek':
                print("ğŸ†“ Using DeepSeek (FREE)")
            elif provider_type == 'local':
                print("ğŸ“ Using Local Templates (FREE)")
            elif provider_type in ['openai', 'gpt', 'chatgpt']:
                print("ğŸ’° Using OpenAI (PAID)")
            elif provider_type in ['claude', 'anthropic']:
                print("ğŸ’° Using Claude (PAID)")

            return provider
        except Exception as e:
            print(f"âš ï¸  Failed to create provider {provider_type}: {e}")
            print("ğŸ”„ Falling back to local provider")
            return LocalLLMProvider()

    @staticmethod
    def list_providers() -> Dict[str, Dict[str, str]]:
        """List all available providers with their details"""
        return {
            "FREE": {
                "gemini": "Google Gemini - 60 req/min (Get key: https://makersuite.google.com/app/apikey)",
                "groq": "Groq - Fast and free (Get key: https://console.groq.com/keys)",
                "deepseek": "DeepSeek - Chinese LLM (Get key: https://platform.deepseek.com/)",
                "local": "Local Templates - No API needed"
            },
            "PAID": {
                "openai": "OpenAI GPT - Requires credits",
                "claude": "Anthropic Claude - Requires credits"
            }
        }


class LLMConfig:
    """Configuration manager for LLM providers"""

    def __init__(self, config_file: str = "llm_config.json"):
        self.config_file = config_file
        self.config = self._load_config()

    def _load_config(self) -> Dict:
        """Load configuration file"""
        try:
            if os.path.exists(self.config_file):
                with open(self.config_file, 'r') as f:
                    return json.load(f)
        except Exception as e:
            print(f"âš ï¸  Failed to load config: {e}")

        return {
            "provider": "local",
            "gemini": {
                "model": "gemini-2.5-flash",
                "api_key": ""
            },
            "groq": {
                "model": "mixtral-8x7b-32768",
                "api_key": ""
            },
            "deepseek": {
                "model": "deepseek-chat",
                "api_key": ""
            },
            "openai": {
                "model": "gpt-4o-mini",
                "api_key": ""
            },
            "claude": {
                "model": "claude-sonnet-4-20250514",
                "api_key": ""
            }
        }

    def save_config(self):
        """Save configuration file"""
        try:
            with open(self.config_file, 'w') as f:
                json.dump(self.config, f, indent=2)
            print(f"âœ… Configuration saved to {self.config_file}")
        except Exception as e:
            print(f"âš ï¸  Failed to save config: {e}")

    def get_provider(self) -> LLMProvider:
        """Get configured LLM provider"""
        provider_type = self.config.get("provider", "local")

        kwargs = {}
        if provider_type in self.config:
            provider_config = self.config[provider_type]
            if "api_key" in provider_config:
                kwargs["api_key"] = provider_config["api_key"]
            if "model" in provider_config:
                kwargs["model"] = provider_config["model"]

        return LLMFactory.create_provider(provider_type, **kwargs)


if __name__ == '__main__':
    print("ğŸ§ª Testing LLM Provider Module\n")
    print("=" * 70)

    print("\nğŸ“‹ Available Providers:")
    providers = LLMFactory.list_providers()

    print("\nğŸ†“ FREE Providers:")
    for name, desc in providers["FREE"].items():
        print(f"   â€¢ {name}: {desc}")

    print("\nğŸ’° PAID Providers:")
    for name, desc in providers["PAID"].items():
        print(f"   â€¢ {name}: {desc}")

    print("\n" + "=" * 70)
    print("\n1ï¸âƒ£  Testing Local Provider:")
    local_provider = LocalLLMProvider()
    desc = local_provider.generate_scenario_description(
        "ADD", "0000", "Addition (A + B)", 16
    )
    print(f"   Scenario description: {desc}\n")

    feature_desc = local_provider.generate_feature_description(
        16, 12, ["ADD", "SUB", "AND", "OR", "XOR"]
    )
    print(f"   Feature description:\n{feature_desc}\n")

    print("2ï¸âƒ£  Testing Configuration Manager:")
    config = LLMConfig()
    print(f"   Current provider: {config.config.get('provider')}")
    provider = config.get_provider()
    print(f"   Provider type: {type(provider).__name__}\n")

    # å¦‚æœæœ‰ Gemini API keyï¼Œæµ‹è¯• Gemini
    if os.getenv("GEMINI_API_KEY"):
        print("3ï¸âƒ£  Testing Gemini Provider:")
        try:
            gemini_provider = LLMFactory.create_provider("gemini")
            desc = gemini_provider.generate_scenario_description(
                "MUL", "1001", "Multiplication (A * B)", 16
            )
            print(f"   Scenario: {desc}\n")
        except Exception as e:
            print(f"   Failed: {e}\n")
    else:
        print("3ï¸âƒ£  Gemini API Key not found. Set GEMINI_API_KEY to test.")

    # å¦‚æœæœ‰ OpenAI API keyï¼Œæµ‹è¯• OpenAI
    if os.getenv("OPENAI_API_KEY"):
        print("4ï¸âƒ£  Testing OpenAI Provider:")
        try:
            openai_provider = LLMFactory.create_provider("openai", model="gpt-4o-mini")
            desc = openai_provider.generate_scenario_description(
                "DIV", "1010", "Division (A / B)", 16
            )
            print(f"   Scenario: {desc}\n")
        except Exception as e:
            print(f"   Failed: {e}\n")
    else:
        print("4ï¸âƒ£  OpenAI API Key not found. Set OPENAI_API_KEY to test.")

    print("=" * 70)
    print("âœ… Testing completed!")
    print("\nğŸ’¡ To use FREE providers:")
    print("   â€¢ Gemini: python script.py --llm-provider gemini --api-key YOUR_KEY")
    print("   â€¢ Groq:   python script.py --llm-provider groq --api-key YOUR_KEY")
    print("   â€¢ Local:  python script.py --llm-provider local (no key needed)")
    print("\nğŸ’¡ To use OpenAI with SDK:")
    print("   â€¢ First install: pip install openai")
    print("   â€¢ Then run: python script.py --llm-provider openai --api-key YOUR_KEY")